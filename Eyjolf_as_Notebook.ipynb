{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_track_length = 10\n",
    "track_smoothing_window_size = 15\n",
    "track_smoothing_std = .5\n",
    "num_discretization_bins = 72\n",
    "\n",
    "num_batches = 300\n",
    "num_hidden = 20  # hochsetzen --> m√§chtigeres Modell\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.load(\"Tracks.npy\")\n",
    "print(np.array(tracks).shape)\n",
    "num_features = tracks[0].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_equal_discretization_bins(data, bins):\n",
    "    \"\"\"\n",
    "    create equally sized bins bewteen minium and maximum value,\n",
    "    excluding start- and endpoint\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : ndarray\n",
    "        feature data of all agents concatenated\n",
    "    bins : int\n",
    "        number of bins\n",
    "    \"\"\"\n",
    "    min_data = min(data)\n",
    "    max_data = max(data)\n",
    "    ret_bins = np.linspace(min_data, max_data, bins, endpoint=False)\n",
    "    ret_bins = np.delete(ret_bins, 0)\n",
    "    return ret_bins\n",
    "\n",
    "# Find out the bins\n",
    "\n",
    "discretization_bins = []\n",
    "concatenated_tracks = np.concatenate(tracks)\n",
    "for feature_i in range(2):\n",
    "    discretization_bins.append(get_equal_discretization_bins(concatenated_tracks[:, feature_i], num_discretization_bins))\n",
    "del(concatenated_tracks)\n",
    "np.save(\"Bins\",discretization_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digitize all values into the bins\n",
    "def digitize_track(track):\n",
    "    for feature_i in range(2):\n",
    "        track[:, feature_i] = np.digitize(track[:, feature_i], discretization_bins[feature_i], right=True)\n",
    "    return track.astype(np.int32)\n",
    "\n",
    "### MODEL SET UP BEGINS\n",
    "\n",
    "np.random.shuffle(tracks)\n",
    "train_tracks = tracks[:int(.8 * len(tracks))]\n",
    "val_tracks = tracks[int(.8 * len(tracks)):]\n",
    "print(len(train_tracks), len(val_tracks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "track_continuous = tf.placeholder(tf.float32, shape=(None, min_track_length, num_features), name='track_continuous')\n",
    "track_discrete = tf.placeholder(tf.int32, shape=(None, min_track_length, num_features), name='track_discrete')\n",
    "\n",
    "num_outputs = 2\n",
    "\n",
    "inputs = track_continuous[:, :-1, :]\n",
    "targets = track_discrete[:, 1:, :num_outputs]\n",
    "\n",
    "with tf.variable_scope('discriminator'):\n",
    "    disc_hidden_0, _ = tf.nn.dynamic_rnn(\n",
    "        tf.contrib.rnn.GRUCell(num_hidden), \n",
    "        inputs, dtype=tf.float32, scope='disc_hidden_0')\n",
    "    disc_hidden_1, _ = tf.nn.dynamic_rnn(\n",
    "        tf.contrib.rnn.GRUCell(num_hidden), \n",
    "        disc_hidden_0, dtype=tf.float32, scope='disc_hidden_1')\n",
    "    disc_hidden_2, _ = tf.nn.dynamic_rnn(\n",
    "        tf.contrib.rnn.GRUCell(num_hidden), \n",
    "        disc_hidden_1, dtype=tf.float32, scope='disc_hidden_2')\n",
    "    # add optional classifier here\n",
    "    \n",
    "with tf.variable_scope('generator'):\n",
    "    gen_hidden_2, _ = tf.nn.dynamic_rnn(\n",
    "        tf.contrib.rnn.GRUCell(num_hidden), \n",
    "        disc_hidden_2, dtype=tf.float32, scope='gen_hidden_2')\n",
    "    gen_hidden_1_input = tf.concat((gen_hidden_2, disc_hidden_1), axis=2)\n",
    "    gen_hidden_1, _ = tf.nn.dynamic_rnn(\n",
    "        tf.contrib.rnn.GRUCell(num_hidden), \n",
    "        gen_hidden_1_input, dtype=tf.float32, scope='gen_hidden_1')\n",
    "    gen_hidden_0_input = tf.concat((gen_hidden_1, disc_hidden_0), axis=2)\n",
    "    gen_hidden_0, _ = tf.nn.dynamic_rnn(\n",
    "        tf.contrib.rnn.GRUCell(num_hidden), \n",
    "        gen_hidden_0_input, dtype=tf.float32, scope='gen_hidden_0')\n",
    "    gen_output, _ = tf.nn.dynamic_rnn(\n",
    "        tf.contrib.rnn.OutputProjectionWrapper(\n",
    "            tf.contrib.rnn.GRUCell(num_hidden), num_outputs * num_discretization_bins), \n",
    "        gen_hidden_0, dtype=tf.float32, scope='gen_features')\n",
    "    gen_output = tf.reshape(gen_output,\n",
    "        (tf.shape(gen_output)[0], tf.shape(gen_output)[1], num_outputs, num_discretization_bins), name='gen_output')\n",
    "\n",
    "# LOSS FUNCTION    \n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=targets, logits=gen_output))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.001)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    update = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subsample(track):\n",
    "    start_idx = np.random.randint(0, track.shape[0] - min_track_length)\n",
    "    end_idx = start_idx + min_track_length\n",
    "    return track[start_idx:end_idx]\n",
    "\n",
    "def data_generator(tracks, size=batch_size):\n",
    "    while True:\n",
    "        indices = np.random.choice(list(range(len(tracks))), replace=False, size=size)\n",
    "        samples = [track for idx, track in enumerate(tracks) if idx in indices]\n",
    "        sampled_tracks = np.array(list(map(subsample, samples)))\n",
    "        digitized_tracks = np.array(list(map(lambda t: digitize_track(t), np.copy(sampled_tracks))))\n",
    "        yield sampled_tracks, digitized_tracks\n",
    "\n",
    "train_gen = data_generator(train_tracks)\n",
    "val_gen = data_generator(val_tracks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "saver.restore(session, 'my-model_1')\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    samples_continuous, samples_discrete = next(train_gen)\n",
    "    batch_loss, _ = session.run([loss, update], feed_dict={track_continuous: samples_continuous,\n",
    "                                              track_discrete: samples_discrete})\n",
    "    train_losses.append(batch_loss)\n",
    "    \n",
    "    samples_continuous, samples_discrete = next(val_gen)\n",
    "    batch_loss = session.run(loss, feed_dict={track_continuous: samples_continuous,\n",
    "                                              track_discrete: samples_discrete})\n",
    "    val_losses.append(batch_loss)\n",
    "    \n",
    "    sys.stdout.write('\\r{}: train-logloss: {:.2f}, val-logloss: {:.2f}'.format(\n",
    "        batch_idx, np.mean(train_losses[-100:]), np.mean(val_losses[-100:])))\n",
    "\n",
    "saver.save(session, 'my-model_1')\n",
    "tf.train.export_meta_graph('meta_graph_1')\n",
    "\n",
    "plt.plot(pd.Series(train_losses).rolling(100).mean(), label='train-logloss')\n",
    "plt.plot(pd.Series(val_losses).rolling(100).mean(), label='val-logloss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = data_generator(val_tracks, size=16)\n",
    "samples_continuous, _ = next(val_gen)\n",
    "\n",
    "disc_hidden = []\n",
    "disc_hidden_2d = []\n",
    "disc_hidden_labels = []\n",
    "\n",
    "for hidden in (disc_hidden_2, disc_hidden_1, disc_hidden_0):\n",
    "    hidden_states = session.run(hidden, feed_dict={track_continuous: samples_continuous})\n",
    "    hidden_states = hidden_states.reshape((-1, hidden_states.shape[-1]))\n",
    "    hidden_states_2d = TSNE().fit_transform(hidden_states.astype(np.float64))\n",
    "    clusterer = HDBSCAN(min_cluster_size=25)\n",
    "    cluster_labels = clusterer.fit_predict(hidden_states)\n",
    "    disc_hidden.append(hidden_states)\n",
    "    disc_hidden_2d.append(hidden_states_2d)\n",
    "    disc_hidden_labels.append(cluster_labels)\n",
    "\n",
    "gen_hidden = []\n",
    "gen_hidden_2d = []\n",
    "gen_hidden_labels = []\n",
    "\n",
    "for hidden in (gen_hidden_2, gen_hidden_1, gen_hidden_0):\n",
    "    hidden_states = session.run(hidden, feed_dict={track_continuous: samples_continuous})\n",
    "    hidden_states = hidden_states.reshape((-1, hidden_states.shape[-1]))\n",
    "    hidden_states_2d = TSNE().fit_transform(hidden_states.astype(np.float64))\n",
    "    clusterer = HDBSCAN(min_cluster_size=25)\n",
    "    cluster_labels = clusterer.fit_predict(hidden_states)\n",
    "    gen_hidden.append(hidden_states)\n",
    "    gen_hidden_2d.append(hidden_states_2d)\n",
    "    gen_hidden_labels.append(cluster_labels)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
    "\n",
    "for idx, (hidden, hidden_2d, clusters) in enumerate(zip(disc_hidden, disc_hidden_2d, disc_hidden_labels)):\n",
    "    cluster_indices = np.where(clusters != -1)[0]\n",
    "    \n",
    "    axes[idx, 0].scatter(hidden_2d[:, 0], hidden_2d[:, 1], alpha=.01)\n",
    "    axes[idx, 0].scatter(hidden_2d[cluster_indices, 0], \n",
    "                         hidden_2d[cluster_indices, 1], \n",
    "                         alpha=.1, c=clusters[cluster_indices],\n",
    "                         cmap=plt.cm.jet)\n",
    "    axes[idx, 0].set_axis_off()\n",
    "    \n",
    "axes[0, 0].set_title('Discriminative')\n",
    "    \n",
    "for idx, (hidden, hidden_2d, clusters) in enumerate(zip(gen_hidden, gen_hidden_2d, gen_hidden_labels)):\n",
    "    cluster_indices = np.where(clusters != -1)[0]\n",
    "    \n",
    "    axes[idx, 1].scatter(hidden_2d[:, 0], hidden_2d[:, 1], alpha=.01)\n",
    "    axes[idx, 1].scatter(hidden_2d[cluster_indices, 0], \n",
    "                         hidden_2d[cluster_indices, 1], \n",
    "                         alpha=.1, c=clusters[cluster_indices],\n",
    "                         cmap=plt.cm.jet)\n",
    "    axes[idx, 1].set_axis_off()\n",
    "    \n",
    "axes[0, 1].set_title('Generative')\n",
    "    \n",
    "_ = plt.suptitle('TSNE of hidden states with HDBSCAN clusters')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
